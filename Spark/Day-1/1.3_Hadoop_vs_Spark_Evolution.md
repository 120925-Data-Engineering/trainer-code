# Topic 1.3: Hadoop vs. Spark

**Theme:** The Evolution from Disk to Memory
**Time Allocation:** 25 Minutes

## 1. The Predecessor: Hadoop MapReduce (Generation 1)

### Context: The Year 2006

* In 2006, RAM was expensive. Hard drives were cheap.
* Google published the "MapReduce" paper, and Yahoo! open-sourced it as Hadoop.
* **Design Goal:** Process data larger than RAM by constantly spilling to disk. Reliability > Speed.

### The MapReduce Data Flow (The "Sandwich")

Explain the rigid 3-step structure of every MapReduce job.

1. **Read:** Load data from HDFS (Disk).
2. **Map Phase:** Process the data.
3. **Intermediate Write:** Write the output of Map to the **local hard drive** of the server.
4. **Shuffle:** Send data across the network to Reducers.
5. **Reduce Phase:** Aggregate the data.
6. **Final Write:** Write the result back to HDFS (Disk).

### The Bottleneck

* **The "Intermediate Write" (Step 3):** This is the killer.
* Even if you just want to do `Step A -> Step B -> Step C`, MapReduce forces you to write to the hard drive between A and B, and between B and C.
* **I/O Cost:** Writing to a spinning HDD takes milliseconds. Reading from RAM takes nanoseconds.
* *Result:* MapReduce spends 90% of its time waiting for the hard drive platter to spin.

---

## 2. The Spark Solution: In-Memory Processing (Generation 2)

### The Architecture Shift

* Spark was born at UC Berkeley (AMPLab) in 2009.
* **Assumption:** RAM is getting cheaper. Let's keep data in memory as long as possible.

### The Spark Data Flow

1. **Read:** Load data from HDFS.
2. **Transformation (Map/Filter):** Process in **RAM**.
3. **Transformation (Map/Filter):** Process in **RAM**. (No disk write!)
4. **Shuffle:** Exchange data across network.
5. **Action (Reduce/Count):** Process in **RAM**.
6. **Final Write:** Write to HDFS.

### The "Lazy" Optimization (Pipelining)

* Because Spark doesn't execute immediately (Lazy Evaluation), it can "fuse" operations.
* *Analogy:*
  * **MapReduce:** "Open file, capitalize letters, save file. Open file, remove vowels, save file."
  * **Spark:** "Open file, capitalize letters AND remove vowels in one pass, save file."

---

## 3. The Killer Use Case: Iterative Algorithms

### The "Loop" Problem

This is the single most important concept for Data Scientists/CS Grads.

* **Scenario:** Machine Learning (e.g., Logistic Regression, K-Means Clustering).
* **The Algorithm:** "Make a guess. Check error. Adjust guess. Repeat 100 times."

### The Comparison

* **Hadoop MapReduce:**
  * Iteration 1: Read from Disk -> Compute -> Write to Disk.
  * Iteration 2: Read from Disk -> Compute -> Write to Disk.
  * ...
  * Iteration 100: Read from Disk -> Compute -> Write to Disk.
  * *Result:* Massive latency.
* **Apache Spark:**
  * Load data into RAM (**Cache**).
  * Iteration 1: Read from RAM -> Compute -> RAM.
  * ...
  * Iteration 100: Read from RAM -> Compute -> RAM.
  * *Result:* **10x to 100x speedup.**

---

## 4. Reality Check: Is Hadoop Dead?

### The Nuance

* **Hadoop MapReduce:** Mostly dead. Very few new projects are written in raw MapReduce. It is "legacy code."
* **Hadoop HDFS (The Storage):** Very much alive.
  * Spark is a compute engine; it needs a storage layer.
  * Many companies use Spark (Compute) on top of HDFS (Storage).
* **Hadoop YARN (The Scheduler):** Very much alive.
  * Spark often runs *inside* YARN to ask for resources.

### Summary Table for Students

| Feature | Hadoop MapReduce | Apache Spark |
| :--- | :--- | :--- |
| **Processing Goal** | Batch Processing | Batch, Streaming, ML, SQL |
| **Data Storage** | Disk-based (Intermediate) | In-Memory (Intermediate) |
| **Performance** | Slow (Heavy I/O) | Fast (10-100x for iterative) |
| **Ease of Use** | High Code (Java Boilerplate) | Low Code (Python/SQL) |
| **Fault Tolerance** | Replication on Disk | RDD Lineage (Recompute) |
