# Topic 1.7: Spark Setup Overview

**Theme:** The Anatomy of the Installation
**Time Allocation:** 15 Minutes

## 1. The Dependencies (The Ingredients)

Before we paste commands into the terminal, explain *what* we are building. We are assembling a Frankenstein monster of languages.

### A. The Engine: Java (JDK 8 or 11)

* **Role:** Since Spark runs on the JVM, we need a Java Runtime Environment (JRE) to run it and a Java Development Kit (JDK) to compile the tasks.
* **The Version:** We stick to **JDK 8 or 11**.
  * *Warning:* Newer Java versions (17+) introduce strict security rules that block Spark from accessing certain memory segments unless you add complex flags. We avoid this complexity today.

### B. The Libraries: Apache Spark Binaries

* **Role:** The actual compiled Scala code, jar files, and shell scripts (`spark-submit`, `pyspark`).
* **The Download:** We don't install this via an `.exe` or `.deb`. We download a `.tgz` (tarball), unzip it, and place it in a folder. It's a "portable" install.

### C. The Interface: Python 3

* **Role:** The language you will type in.
* **The Connector:** We need the `pyspark` pip library to handle the client-side logic.

---

## 2. The Environment Variables (The Glue)

This is where 90% of students fail. Explain **why** we edit `.bashrc`.

### Variable 1: `JAVA_HOME`

* **The Question:** Spark starts up and asks, "Where is the `java` command?"
* **The Answer:** `export JAVA_HOME=/usr/lib/jvm/...`
* *Analogy:* This is like giving Spark the address to the Power Plant. Without it, the lights don't turn on.

### Variable 2: `SPARK_HOME`

* **The Question:** Python asks, "I need to import the Py4J library. Where is it?"
* **The Answer:** `export SPARK_HOME=/opt/spark`
* *Reason:* Python looks inside `$SPARK_HOME/python/lib` to find the code that lets it talk to Java.

### Variable 3: `PATH`

* **The Question:** You type `pyspark` in the shell. Linux asks, "What program is that?"
* **The Answer:** `export PATH=$PATH:$SPARK_HOME/bin`
* *Reason:* This adds the Spark `bin` folder to the list of places Linux looks for executable programs.

---

## 3. The WSL2 Specific Challenges (The "Gotchas")

Since we are on Windows Subsystem for Linux (WSL2), we face unique networking quirks.

### The "Dynamic IP" Problem

* **Scenario:** WSL2 changes its internal IP address every time you restart Windows.
* **The Crash:** Spark tries to bind to an IP address to listen for tasks. If it picks the wrong network adapter (e.g., the Windows WiFi adapter instead of the Linux virtual adapter), it hangs.
* **The Fix:** `export SPARK_LOCAL_IP=127.0.0.1`
* *Meaning:* We force Spark to only listen on "localhost" (the internal loopback). We don't need external connections for this class.

### The File System Boundary

* **Linux Drive:** `/home/user/` (Fast, Native).
* **Windows Drive:** `/mnt/c/Users/...` (Slow, Translated).
* **Advice:** "For this week, keep your data and scripts inside your Linux home directory (`~`). Do not try to read files directly from your Windows Desktop unless you want permission errors."

---

## 4. Transition to Lab

* **Instruction:** "We are now moving from Theory to Practice."
* **Next Step:** "Open your terminals. We will run the installation script step-by-step."
