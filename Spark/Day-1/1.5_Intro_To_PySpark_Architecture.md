# Topic 1.5: Introduction to PySpark

**Theme:** Bridging the Language Gap (Python <-> JVM)
**Time Allocation:** 25 Minutes

## 1. The Fundamental Problem (The Language Barrier)

### The Two Worlds

* **Spark's Native Tongue:** Scala (runs on the Java Virtual Machine). It expects static typing and compiled bytecode.
* **Your Native Tongue:** Python (Dynamic, Interpreted).
* **The Challenge:** How does a Python script control a distributed Java cluster? They cannot share memory address spaces.

---

## 2. The Architecture: Py4J (The Translator)

### What is PySpark?

* PySpark is **not** a rewrite of Spark in Python.
* It is a **wrapper** library.
* It relies on a library called **Py4J** (Python for Java).

### The Mechanics (The "Socket")

1. **The Launch:** When you type `pyspark`, you actually launch **two** processes on your machine:
    * A Python Process (Your shell).
    * A JVM Process (The Spark Driver).
2. **The Connection:** PySpark opens a local TCP socket (usually port `25333` or similar) between Python and the JVM.
3. **The API:** When you run `spark.read.csv(...)`:
    * Python serializes this command string.
    * Sends it over the socket.
    * The JVM receives it, parses it, and executes the actual Java code to read the file.
    * The JVM returns a reference (ID) of the resulting DataFrame back to Python.

> **Analogy:** You (Python) are the architect in the office. The construction crew (JVM) is on the site. You don't lay the bricks yourself; you send blueprints (API calls) over the phone (Socket).

---

## 3. The Performance Trap: Native vs. UDFs

This is the most critical technical concept for performance tuning later.

### Scenario A: Built-in Functions (The "Good" Way)

* **Code:** `df.filter(col("age") > 21)`
* **Execution:**
  * Spark sees `col("age") > 21`.
  * It translates this directly into a **SQL Expression** in the JVM.
  * The data **stays inside the JVM**. It never touches Python.
  * **Speed:** Extremely fast (Native Java speed).

### Scenario B: Python UDFs (The "Bad" Way)

* **Code:** `df.rdd.map(lambda x: x.age + 1)`
* **The Problem:** The JVM does not know how to execute a Python `lambda`.
* **The Execution Horror:**
    1. The Executor (JVM) takes a row of data.
    2. It saves it to a buffer.
    3. It spins up a generic **Python Sub-process** (pyspark.daemon).
    4. It **Serializes** (Pickles) the data and pipes it to the Python process.
    5. Python runs `x + 1`.
    6. Python **Serializes** the result and pipes it back to the JVM.
* **The Cost:** Serialization is CPU-heavy. You are paying a tax on every single row.

---

## 4. Best Practices for CS Grads

### The Golden Rules

1. **Avoid Python Loops:** Never write `for row in data:`. That happens on the Driver (local) and pulls all data to your laptop.
2. **Prefer the DataFrame API:** Use `spark.sql` or `df.select` functions. These are optimized by the Catalyst Optimizer (Java).
3. **Minimize UDFs:** Only use Python User Defined Functions if a native Spark function does not exist.

### Summary

* **PySpark** is just a remote control.
* **The JVM** is the engine.
* Keep the data in the engine (JVM) as much as possible to avoid the "Serialization Tax."
