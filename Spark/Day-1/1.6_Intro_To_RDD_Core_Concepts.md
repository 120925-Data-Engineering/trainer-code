# Topic 1.6: Introduction to RDDs
**Theme:** The "Assembly Language" of Spark
**Time Allocation:** 30 Minutes

## 1. Definition: The Atomic Unit

### What is an RDD?

* **Acronym:** **R**esilient **D**istributed **D**ataset.
* **The Concept:** It is the primary data abstraction in Spark. Even if you use high-level DataFrames or SQL later, they are all compiled down to RDDs eventually.

### Breaking Down the Acronym

1.  **Dataset:** It is a collection of objects.
    * In PySpark, an RDD can hold *any* Python object (Integer, String, Tuple, Custom Class).
    * *Note:* Unlike DataFrames, RDDs do not have a schema. They are "schemaless."
2.  **Distributed:** The data is not stored on one machine. It is sliced into **Partitions**.
3.  **Resilient:** It can recover from failure automatically (explained below).

---

## 2. Partitioning (The Physical Layout)

### How Spark Sees Data

* **The Slice:** When you load a 10GB file, Spark does not load it as one object.
* **Default Behavior:** It splits the file into chunks (typically 128MB, matching the HDFS block size).
* **The Distribution:**
    * Partition 1 -> Node A
    * Partition 2 -> Node B
    * Partition 3 -> Node C
* **Parallelism:** The number of partitions determines your parallelism.
    * *Rule of Thumb:* If you have 10 CPU cores in your cluster but only 2 partitions, 8 cores will sit idle.



---

## 3. Fault Tolerance: Lineage (The "Resilient" Part)

This is the most "Computer Science" part of the lecture. Compare it to Hadoop.

### The Problem: Node Failure

* In a cluster of 1,000 nodes, hardware failure is guaranteed.
* **Scenario:** You are 90% done with a 4-hour job. Node 5 crashes and takes Partition 5 with it.

### The Old Solution (Hadoop/RAID)

* **Replication:** Keep 3 copies of every piece of data on disk.
* *Cost:* High storage overhead. High I/O penalty.

### The Spark Solution (Lineage)

* **Recomputation:** Spark does **not** replicate the data in RAM.
* **The Recipe:** Instead, it remembers the **steps** used to create the data.
* **The Lineage Graph (DAG):**
    * `RDD_Final` depends on `RDD_Filtered`.
    * `RDD_Filtered` depends on `RDD_Mapped`.
    * `RDD_Mapped` depends on `File_Input`.
* **Recovery:** If Partition 5 is lost, Spark looks at the DAG and re-runs the transformation steps *only for Partition 5*.
* *Benefit:* Saves RAM. Data is only recomputed if absolutely necessary.



---

## 4. Immutability & Lazy Evaluation (The Execution Model)

### Immutability

* **Rule:** You **cannot** modify an RDD in place.
    * *Pandas:* `df['age'] = df['age'] + 1` (In-place modification).
    * *Spark:* `new_rdd = old_rdd.map(...)` (Create new pointer).
* **Why?** In a distributed system, mutable state requires **Locks** and **Synchronization** (Mutex). By making data immutable, Spark eliminates race conditions. Workers can read data without fighting each other.

### Lazy Evaluation

* **Transformations (The Recipe):**
    * Operations that define a new RDD (e.g., `map`, `filter`, `join`).
    * **Behavior:** These return immediately. Spark just adds a step to the plan (DAG). No data is touched.
* **Actions (The Trigger):**
    * Operations that return a result to the Driver or disk (e.g., `count`, `collect`, `save`).
    * **Behavior:** Spark looks at the accumulated plan, optimizes it, and executes it.
* **The "Why" (Optimization):**
    * If you write `load().map().filter().take(10)`, Spark knows it only needs to process enough data to find 10 records. It won't process the whole Petabyte file. This is impossible with "Eager" execution.

### Summary Table for Students

| Feature | RDD Property |
| :--- | :--- |
| **Data Storage** | Partitioned across RAM/Disk |
| **Failure Recovery** | Lineage (Replay the log) |
| **Modification** | Impossible (Immutable) |
| **Execution** | Lazy (Wait for Action) |